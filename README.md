<h1>ğŸ Darkhorse Genesis</h1><p><strong>Experimental Mixture-of-Experts (MoE) Model</strong> |  <strong>Research Lab:</strong> <a href="https://alchymia.ai" title="null">Alchymia AI</a></p><h2>ğŸš€ Overview</h2><p><strong>Darkhorse Genesis</strong> is a high-efficiency "Large Language Model" (LLM) utilizing a proprietary sparsely activated <strong>Mixture-of-Experts (MoE)</strong> architecture. Optimized with <strong>Multi-Head Latent Attention (MLA)</strong>, it delivers frontier-level reasoning across specialized domains with minimal VRAM overhead.</p><h3>ğŸ’ Key Specifications</h3><ul><li><p><strong>Total Parameters:</strong> 5B (Scalable from 1B to 500B)</p></li><li><p><strong>Active Parameters:</strong> ~1.2B per token (for 5B base)</p></li><li><p><strong>Expert Count:</strong> 64 specialized experts across 10 domains</p></li><li><p><strong>Context Window:</strong> 128K tokens (MLA Optimized)</p></li><li><p><strong>Target Hardware:</strong> Consumer GPUs (12GB+ VRAM) or Apple Silicon (Metal)</p></li></ul><h2>ğŸ“Š Model Comparisons</h2><p>Darkhorse Genesis is designed to punch above its weight <span class="selected">class</span> by using sparse activation. Below is how it compares to standard dense LLMs.</p><table><tbody><tr><th><p>Metric</p></th><th><p>Phi-3 Mini (Microsoft)</p></th><th><p>Llama 3.2 3B (Meta)</p></th><th><p><strong>Darkhorse Genesis</strong></p></th></tr><tr><td><p><strong>Architecture</strong></p></td><td><p>Dense Transformer</p></td><td><p>Dense Transformer</p></td><td><p><strong>MLA + Alchymia MoE</strong></p></td></tr><tr><td><p><strong>Total Params</strong></p></td><td><p>3.8B</p></td><td><p>3.2B</p></td><td><p><strong>5B</strong></p></td></tr><tr><td><p><strong>Active Params</strong></p></td><td><p>3.8B</p></td><td><p>3.2B</p></td><td><p><strong>~1.2B</strong></p></td></tr><tr><td><p><strong>Throughput</strong></p></td><td><p>Baseline</p></td><td><p>Baseline</p></td><td><p><strong>~3x Faster</strong></p></td></tr><tr><td><p><strong>KV Cache</strong></p></td><td><p>Standard</p></td><td><p>Standard</p></td><td><p><strong>Ultra-Low (MLA)</strong></p></td></tr><tr><td><p><strong>Specialization</strong></p></td><td><p>Generalist</p></td><td><p>Generalist</p></td><td><p><strong>Domain-ID Isolated</strong></p></td></tr></tbody></table><h2>ğŸ›  Installation &amp; Setup</h2><h3>ğŸ“‹ Prerequisites</h3><ol><li><p><strong>Rust Toolchain:</strong> <a href="https://rustup.rs/" title="null">Install Rust</a></p></li><li><p><strong>Libtorch:</strong> Required for <code>tch-rs</code>. <a href="https://pytorch.org/" title="null">Download PyTorch C++ Libs</a>.</p></li></ol><p>&lt;!-- end list --&gt;</p><pre><code># Set environment variables for Libtorch (Linux/macOS)
export LIBTORCH=/path/to/libtorch
export LD_LIBRARY_PATH=$LIBTORCH/lib:$LD_LIBRARY_PATH
<br class="ProseMirror-trailingBreak"></code></pre><h3>âš¡ Quick Start</h3><pre><code>git clone [https://github.com/alchymia-ai/darkhorse-genesis.git](https://github.com/alchymia-ai/darkhorse-genesis.git)
cd darkhorse-genesis
cargo build --release
<br class="ProseMirror-trailingBreak"></code></pre><h2>ğŸ— Multi-Scale Configurations</h2><p>The engine now supports dynamic parameter scaling. Configuration files are located in the <code>parameters/</code> directory.</p><table><tbody><tr><th><p>Scale</p></th><th><p>Command Flag</p></th><th><p>Target Architecture</p></th></tr><tr><td><p><strong>1B</strong></p></td><td><p><code>--params 1b</code></p></td><td><p>Edge Devices / Mobile</p></td></tr><tr><td><p><strong>5B</strong></p></td><td><p><code>--params 5b</code></p></td><td><p><strong>Default</strong> - Consumer GPUs</p></td></tr><tr><td><p><strong>30B+</strong></p></td><td><p><code>--params 30b</code></p></td><td><p>Multi-GPU Clusters</p></td></tr><tr><td><p><strong>500B</strong></p></td><td><p><code>--params 500b</code></p></td><td><p>Frontier Research / HPC</p></td></tr></tbody></table><h2>ğŸ§  Inference</h2><p>Generate text directly via the Command Line Interface. The default profile uses a 512-hidden size configuration for rapid local execution.</p><pre><code># Direct CLI Inference with default settings
cargo run --release -- --prompt "Explain the laws of thermodynamics."

# Inference using a specific parameter scale
cargo run --release -- inference --params 1b --prompt "Write a Rust function."
<br class="ProseMirror-trailingBreak"></code></pre><h2>ğŸ§¬ Data Pipeline &amp; Training</h2><p>Darkhorse Genesis requires structured data to train its specialized expert domains (Math, Code, Legal, etc.).</p><h3>1. Local Data Preparation</h3><p>Populate a <code>data/</code> directory with <code>.jsonl</code> files. Each entry should include a domain tag to assist the router.</p><p><strong>Format (<code>train_data.jsonl</code>):</strong></p><pre><code>{"domain": "code", "text": "fn main() { println!(\"Alchymia AI\"); }"}
{"domain": "math", "text": "The Riemann hypothesis is a conjecture that..."}
<br class="ProseMirror-trailingBreak"></code></pre><h3>2. Training Execution</h3><p>To initiate training with a specific parameter scale and the <strong>Auxiliary-Loss-Free (ALF)</strong> balancing strategy:</p><pre><code># Train a 1B parameter model
cargo run --release -- train --params 1b --data ./data/train_data.jsonl

# Train the flagship 5B model
cargo run --release -- train --params 5b --data ./data/train_data.jsonl
<br class="ProseMirror-trailingBreak"></code></pre><h2>ğŸš¢ Production Deployment</h2><h3>ğŸ³ Containerization (Docker)</h3><pre><code>FROM nvidia/cuda:12.1.0-base-ubuntu22.04

LABEL maintainer="Alchymia AI"
COPY ./target/release/darkhorse-genesis /usr/local/bin/

ENTRYPOINT ["darkhorse-genesis"]
<br class="ProseMirror-trailingBreak"></code></pre><h2>ğŸ“„ License</h2><p>Copyright Â© 2024 <strong>Alchymia AI</strong>.</p><blockquote><p>[!IMPORTANT]
For technical inquiries, please refer to the <a href="" title="null">Darkhorse Genesis Release Paper</a>.</p></blockquote>