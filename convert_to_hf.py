import torch
from safetensors.torch import save_file
import json
import os

def convert_alchymia_to_hf(input_pt_path, output_dir):
    """
    Converts AlchymiaGen (Rust tch-rs) weights to HuggingFace Safetensors.
    Handles both standard pickles and TorchScript (.ot) archives.
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    print(f"Loading raw weights from {input_pt_path}...")
    try:
        # Load the file. If it's a .ot file, PyTorch might load it as a ScriptModule.
        # We set weights_only=False because TorchScript/Zip archives require the full loader.
        data = torch.load(input_pt_path, map_location="cpu", weights_only=False)
    except Exception as e:
        print(f"Error loading weights: {e}")
        return

    hf_state_dict = {}

    # Check if loaded data is a dictionary (standard) or a ScriptModule (TorchScript/.ot)
    if isinstance(data, dict):
        print("Detected standard state_dict format.")
        source_dict = data
    elif hasattr(data, "named_parameters"):
        print("Detected TorchScript/RecursiveScriptModule format. Extracting parameters...")
        # Extract both parameters and buffers (like LayerNorm stats)
        source_dict = {name: param.data for name, param in data.named_parameters()}
        source_dict.update({name: buf.data for name, buf in data.named_buffers()})
    else:
        print(f"Unknown data type: {type(data)}. Attempting state_dict() call...")
        try:
            source_dict = data.state_dict()
        except:
            print("Failed to extract state_dict. Weights might be in an unsupported format.")
            return

    # Map names and clean up tch-rs artifacts
    for name, tensor in source_dict.items():
        # Rust paths often start with 'root.' or contain internal tch prefixes
        clean_name = name.replace("root.", "")
        hf_state_dict[clean_name] = tensor

    # 3. Create the config.json (matching src/config.rs values)
    config = {
        "architectures": ["DarkhorseGenesisForCausalLM"],
        "model_type": "darkhorse_genesis",
        "hidden_size": 512,
        "num_attention_heads": 8,
        "num_hidden_layers": 6,
        "kv_lora_rank": 64,
        "q_lora_rank": 128,
        "moe_num_experts": 8,
        "moe_top_k": 2,
        "vocab_size": 32768,
        "torch_dtype": "float32",
        "transformers_version": "5.0.0.dev0" 
    }

    # Tokenizer Config (This fixes the legacy=False warning in vLLM)
    tokenizer_config = {
        "add_bos_token": True,
        "add_eos_token": False,
        "bos_token": "<s>",
        "clean_up_tokenization_spaces": False,
        "eos_token": "</s>",
        "legacy": False, 
        "model_max_length": 131072,
        "pad_token": "</s>",
        "spm_char_coverage": 1.0,
        "tokenizer_class": "LlamaTokenizer",
        "unk_token": "<unk>",
        "chat_template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"
    }

     # This creates a basic structure so AutoTokenizer doesn't look for a .model file
    tokenizer_json = {
        "version": "1.0",
        "truncation": None,
        "padding": None,
        "added_tokens": [
            {"id": 0, "content": "<s>", "single_word": False, "lstrip": False, "rstrip": False, "normalized": False, "special": True},
            {"id": 1, "content": "</s>", "single_word": False, "lstrip": False, "rstrip": False, "normalized": False, "special": True},
            {"id": 2, "content": "<unk>", "single_word": False, "lstrip": False, "rstrip": False, "normalized": False, "special": True}
        ],
        "model": {
            "type": "BPE",
            "vocab": {"<s>": 0, "</s>": 1, "<unk>": 2},
            "merges": []
        }
    }


    # 4. Save Safetensors
    print(f"Saving {len(hf_state_dict)} tensors to Safetensors...")
    save_file(hf_state_dict, os.path.join(output_dir, "model.safetensors"), metadata={"format": "pt"})

    # 5. Save Config
    with open(os.path.join(output_dir, "config.json"), "w") as f:
        json.dump(config, f, indent=2)

    with open(os.path.join(output_dir, "tokenizer_config.json"), "w") as f:
        json.dump(tokenizer_config, f, indent=2)

    with open(os.path.join(output_dir, "tokenizer.json"), "w") as f:
        json.dump(tokenizer_json, f, indent=2)

    print(f"âœ… Success! HuggingFace model files are ready in: {output_dir}")

if __name__ == "__main__":
    # Ensure this matches the file generated by src/train.rs
    convert_alchymia_to_hf("model.ot", "./hf_model_out")