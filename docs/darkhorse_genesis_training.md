<h1>AlchymiaGen Training Guide: Step-by-Step</h1><p>This guide outlines the procedure for training the <strong>Darkhorse Genesis</strong> model using the Alchymia AI research framework.</p><h2><span class="selected">Step</span> 1: Environment Preparation</h2><p>Ensure your environment meets the high-performance requirements for MoE training.</p><ol><li><p><strong>Libtorch:</strong> Download the C++ distribution of PyTorch (Libtorch) and set your paths.</p><p><strong>For Linux:</strong></p><pre><code>export LIBTORCH=/path/to/libtorch
export LD_LIBRARY_PATH=$LIBTORCH/lib:$LD_LIBRARY_PATH
<br class="ProseMirror-trailingBreak"></code></pre><p><strong>For macOS (Intel/M1/M2/M3):</strong>
The dynamic linker (<code>dyld</code>) requires the <code>DYLD_LIBRARY_PATH</code> to find the <code>.dylib</code> files.</p><pre><code>export LIBTORCH=/path/to/libtorch
export DYLD_LIBRARY_PATH=$LIBTORCH/lib:$DYLD_LIBRARY_PATH
<br class="ProseMirror-trailingBreak"></code></pre><p><em>Note: If you still encounter <code>Library not loaded</code>, ensure you are using the Libtorch version that matches your architecture (e.g., <code>libtorch-macos-arm64</code> for Apple Silicon).</em></p></li><li><p><strong>Hardware:</strong> Minimum 24GB VRAM (RTX 3090/4090 or A100/H100) is recommended for 5B parameter training, even with sparse activation. For macOS users, ensure you have sufficient Unified Memory.</p></li></ol><h2>Step 2: Data Preparation (<code>.jsonl</code>)</h2><p>Darkhorse Genesis uses <strong>Domain-Tagged Data</strong>. This is critical for the router to learn which experts to activate for specific subjects.</p><p>Create a <code>data/train.jsonl</code> file with the following structure:</p><pre><code>{"domain": "code", "text": "fn main() { println!(\"Hello Alchymia\"); }"}
{"domain": "math", "text": "The derivative of x^2 is 2x."}
{"domain": "legal", "text": "Pursuant to Section 4.2 of the agreement..."}

<br class="ProseMirror-trailingBreak"></code></pre><h2>Step 3: Configuring Hyperparameters</h2><p>Open <code>src/config.rs</code> and verify the training constants:</p><ul><li><p><code>moe_num_experts</code>: 64</p></li><li><p><code>moe_top_k</code>: 4 (Activates ~1.2B params per token)</p></li><li><p><code>num_next_tokens</code>: 1 (Enables MTP signal)</p></li></ul><h2>Step 4: The Training Execution</h2><p>Run the unified binary in training mode. This activates the <code>Trainer</code> logic in <code>src/train.rs</code>.</p><pre><code># Compile and run the training session
cargo run --release -- train --data data/train.jsonl
<br class="ProseMirror-trailingBreak"></code></pre><h3>What happens during this step?</h3><ol><li><p><strong>Initialization:</strong> The <code>VarStore</code> allocates tensors on the GPU (or MPS on Mac).</p></li><li><p><strong>Forward Pass:</strong> The model processes a batch through <strong>MLA</strong> (Attention) and <strong>MoE</strong> (Experts).</p></li><li><p><strong>ALF Balancing:</strong> The system monitors expert utilization. If the "Legal" expert is underused, it applies a <strong>Bias Boost</strong> in <code>src/moe.rs</code> to attract more tokens without adding auxiliary loss.</p></li><li><p><strong>MTP Objective:</strong> The model calculates loss for the next token <em>and</em> the predicted future tokens simultaneously.</p></li></ol><h2>Step 5: Checkpointing &amp; Export</h2><p>Once training completes, the model weights are stored in the <code>tch</code> format (<code>.ot</code>). To use the model in other engines (vLLM/SGLang):</p><ol><li><p><strong>Export Raw Weights:</strong> Ensure <code>vs.save("model.ot")</code> is called at the end of training.</p></li><li><p><strong>Convert to Safetensors:</strong> Use the provided conversion script:</p><pre><code>python convert_to_hf.py

<br class="ProseMirror-trailingBreak"></code></pre></li></ol><h2>Step 6: Validation (Inference)</h2><p>Test the newly trained weights using the inference engine:</p><pre><code>cargo run --release -- --prompt "Write a Rust function to sort an array."

<br class="ProseMirror-trailingBreak"></code></pre><h2>Troubleshooting Training</h2><ul><li><p><strong>OOM (Out of Memory):</strong> Reduce <code>batch_size</code> in <code>src/train.rs</code> or reduce <code>kv_lora_rank</code> in <code>src/config.rs</code>.</p></li><li><p><strong>Expert Collapse:</strong> If the model outputs gibberish, check if one expert is taking 100% of the load; increase the ALF bias learning rate if necessary.</p></li><li><p><strong>Library Not Loaded (macOS):</strong> If you see <code>dyld: Library not loaded</code>, double-check that <code>DYLD_LIBRARY_PATH</code> includes the absolute path to your libtorch <code>lib</code> folder. You may also need to run <code>xattr -r -d com.apple.quarantine /path/to/libtorch</code> if macOS is blocking the binaries.</p></li></ul>